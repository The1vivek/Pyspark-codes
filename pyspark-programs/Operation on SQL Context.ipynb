{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # only run after findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataRows=[(1,'abc','uk',10000),(1,'xyz','canada',50000),(3,'abcd','us',20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataColumns=['empId','name','country','salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sqlContext.createDataFrame(dataRows,dataColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-------+------+\n",
      "|empId|name|country|salary|\n",
      "+-----+----+-------+------+\n",
      "|    1| abc|     uk| 10000|\n",
      "|    1| xyz| canada| 50000|\n",
      "|    3|abcd|     us| 20000|\n",
      "+-----+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=[('Alice',1)]\n",
    "sqlContext.createDataFrame(l).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Alice', Age=1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.createDataFrame(l,['Name','Age']).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=12, name='Alice')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d=[{'name':'Alice','age':12}]\n",
    "sqlContext.createDataFrame(d).collect()#depricated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Alice', Age=1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=sc.parallelize(l)\n",
    "df1=sqlContext.createDataFrame(rdd,['Name','Age'])\n",
    "df1.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Alice', Age=1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "Person=Row('Name','Age')\n",
    "person=rdd.map(lambda x:Person(*x))\n",
    "df2=sqlContext.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', Age=1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema=StructType([\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"Age\",IntegerType(),True)\n",
    "])\n",
    "df3=sqlContext.createDataFrame(rdd,schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Alice', Age=1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.createDataFrame(df1.toPandas()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a='Alice', b=1)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.createDataFrame(rdd,\"a:string,b:int\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='Alice')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd=rdd.map(lambda x:x[0])\n",
    "sqlContext.createDataFrame(rdd,\"string\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,\"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.dropTempTable(\"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'200'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.getConf(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=1), Row(id=3), Row(id=5)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.range(1,7,2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(df,\"table1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(country='uk', salary=10000),\n",
       " Row(country='canada', salary=50000),\n",
       " Row(country='us', salary=20000)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqlContext.sql(\"select country,salary from table1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"table1\" in sqlContext.tableNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SQLContext in module pyspark.sql.context object:\n",
      "\n",
      "class SQLContext(builtins.object)\n",
      " |  SQLContext(sparkContext, sparkSession=None, jsqlContext=None)\n",
      " |  \n",
      " |  The entry point for working with structured data (rows and columns) in Spark, in Spark 1.x.\n",
      " |  \n",
      " |  As of Spark 2.0, this is replaced by :class:`SparkSession`. However, we are keeping the class\n",
      " |  here for backward compatibility.\n",
      " |  \n",
      " |  A SQLContext can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  \n",
      " |  :param sparkContext: The :class:`SparkContext` backing this SQLContext.\n",
      " |  :param sparkSession: The :class:`SparkSession` around which this SQLContext wraps.\n",
      " |  :param jsqlContext: An optional JVM Scala SQLContext. If set, we do not instantiate a new\n",
      " |      SQLContext in the JVM, instead we make all calls to this object.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, sparkContext, sparkSession=None, jsqlContext=None)\n",
      " |      Creates a new SQLContext.\n",
      " |      \n",
      " |      >>> from datetime import datetime\n",
      " |      >>> sqlContext = SQLContext(sc)\n",
      " |      >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |      ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |      ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |      >>> df = allTypes.toDF()\n",
      " |      >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |      >>> sqlContext.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |      ...            'from allTypes where b and i > 0').collect()\n",
      " |      [Row((i + CAST(1 AS BIGINT))=2, (d + CAST(1 AS DOUBLE))=2.0, (NOT b)=False, list[1]=2,             dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |      >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |      [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  cacheTable(self, tableName)\n",
      " |      Caches the specified table in-memory.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  clearCache(self)\n",
      " |      Removes all cached tables from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of :class:`Row`,\n",
      " |      or :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      " |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      :param data: an RDD of any kind of SQL data representation(e.g. :class:`Row`,\n",
      " |          :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\n",
      " |          :class:`pandas.DataFrame`.\n",
      " |      :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is None.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\n",
      " |          We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :param verifySchema: verify data types of every row against schema.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 2.0\n",
      " |         The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\n",
      " |         datatype string after 2.0.\n",
      " |         If it's not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |         :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\n",
      " |      \n",
      " |      .. versionchanged:: 2.1\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> sqlContext.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> sqlContext.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> sqlContext.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> sqlContext.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = sqlContext.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = sqlContext.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = sqlContext.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> sqlContext.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  createExternalTable(self, tableName, path=None, source=None, schema=None, **options)\n",
      " |      Creates an external table based on the dataset in a data source.\n",
      " |      \n",
      " |      It returns the DataFrame associated with the external table.\n",
      " |      \n",
      " |      The data source is specified by the ``source`` and a set of ``options``.\n",
      " |      If ``source`` is not specified, the default data source configured by\n",
      " |      ``spark.sql.sources.default`` will be used.\n",
      " |      \n",
      " |      Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\n",
      " |      created external table.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  dropTempTable(self, tableName)\n",
      " |      Remove the temp table from catalog.\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> sqlContext.dropTempTable(\"table1\")\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  getConf(self, key, defaultValue=<no value>)\n",
      " |      Returns the value of Spark SQL configuration property for the given key.\n",
      " |      \n",
      " |      If the key is not set and defaultValue is set, return\n",
      " |      defaultValue. If the key is not set and defaultValue is not set, return\n",
      " |      the system default value.\n",
      " |      \n",
      " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n",
      " |      '200'\n",
      " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n",
      " |      '10'\n",
      " |      >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", u\"50\")\n",
      " |      >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n",
      " |      '50'\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new SQLContext as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared SparkContext and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numPartitions: the number of partitions of the DataFrame\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> sqlContext.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  registerDataFrameAsTable(self, df, tableName)\n",
      " |      Registers the given :class:`DataFrame` as a temporary table in the catalog.\n",
      " |      \n",
      " |      Temporary tables exist only during the lifetime of this instance of :class:`SQLContext`.\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  registerFunction(self, name, f, returnType=None)\n",
      " |      An alias for :func:`spark.udf.register`.\n",
      " |      See :meth:`pyspark.sql.UDFRegistration.register`.\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.3.0. Use :func:`spark.udf.register` instead.\n",
      " |      \n",
      " |      .. versionadded:: 1.2\n",
      " |  \n",
      " |  registerJavaFunction(self, name, javaClassName, returnType=None)\n",
      " |      An alias for :func:`spark.udf.registerJavaFunction`.\n",
      " |      See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\n",
      " |      \n",
      " |      .. note:: Deprecated in 2.3.0. Use :func:`spark.udf.registerJavaFunction` instead.\n",
      " |      \n",
      " |      .. versionadded:: 2.1\n",
      " |  \n",
      " |  setConf(self, key, value)\n",
      " |      Sets the given Spark SQL configuration property.\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table or view as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  tableNames(self, dbName=None)\n",
      " |      Returns a list of names of tables in the database ``dbName``.\n",
      " |      \n",
      " |      :param dbName: string, name of the database to use. Default to the current database.\n",
      " |      :return: list of table names, in string\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> \"table1\" in sqlContext.tableNames()\n",
      " |      True\n",
      " |      >>> \"table1\" in sqlContext.tableNames(\"default\")\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  tables(self, dbName=None)\n",
      " |      Returns a :class:`DataFrame` containing names of tables in the given database.\n",
      " |      \n",
      " |      If ``dbName`` is not specified, the current database will be used.\n",
      " |      \n",
      " |      The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\n",
      " |      (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\n",
      " |      \n",
      " |      :param dbName: string, name of the database to use.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\n",
      " |      >>> df2 = sqlContext.tables()\n",
      " |      >>> df2.filter(\"tableName = 'table1'\").first()\n",
      " |      Row(database='', tableName='table1', isTemporary=True)\n",
      " |      \n",
      " |      .. versionadded:: 1.3\n",
      " |  \n",
      " |  uncacheTable(self, tableName)\n",
      " |      Removes the specified table from the in-memory cache.\n",
      " |      \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  getOrCreate(sc) from builtins.type\n",
      " |      Get the existing SQLContext or create a new one with given SparkContext.\n",
      " |      \n",
      " |      :param sc: SparkContext\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrameReader`\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamReader`\n",
      " |      \n",
      " |      >>> text_sdf = sqlContext.readStream.text(tempfile.mkdtemp())\n",
      " |      >>> text_sdf.isStreaming\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      :return: :class:`UDFRegistration`\n",
      " |      \n",
      " |      .. versionadded:: 1.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sqlContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
